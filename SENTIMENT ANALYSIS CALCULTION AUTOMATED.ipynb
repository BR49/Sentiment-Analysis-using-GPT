{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snscrape\n",
      "  Downloading snscrape-0.6.0.20230303-py3-none-any.whl (71 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\programdata\\anaconda3\\lib\\site-packages (from snscrape) (2.26.0)\n",
      "Requirement already satisfied: lxml in c:\\programdata\\anaconda3\\lib\\site-packages (from snscrape) (4.6.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from snscrape) (4.10.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from snscrape) (3.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n",
      "Installing collected packages: snscrape\n",
      "Successfully installed snscrape-0.6.0.20230303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lrd (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n",
      "                         Date             User  \\\n",
      "0   2023-02-28 23:59:59+00:00     jmbushwrites   \n",
      "1   2023-02-28 23:59:56+00:00   cryptobits_org   \n",
      "2   2023-02-28 23:59:52+00:00          romoolo   \n",
      "3   2023-02-28 23:59:52+00:00      BitcoinEnso   \n",
      "4   2023-02-28 23:59:51+00:00  AdamBitcoinP2P1   \n",
      "..                        ...              ...   \n",
      "995 2023-02-28 23:23:46+00:00     weeblueghost   \n",
      "996 2023-02-28 23:23:44+00:00       precio_btc   \n",
      "997 2023-02-28 23:23:43+00:00        Skype_Ban   \n",
      "998 2023-02-28 23:23:41+00:00  michelleweekley   \n",
      "999 2023-02-28 23:23:40+00:00     ImNotTheWolf   \n",
      "\n",
      "                                                 Tweet  \n",
      "0     Sound money matters.\\n\\n#Bitcoin is sound money.  \n",
      "1    Visa and Mastercard are pausing crypto product...  \n",
      "2    ❌ do not ever bet against El Salvador 🇸🇻 nor #...  \n",
      "3    @trustmachinesco is dedicated to building and ...  \n",
      "4                   @windyseob We’ll share one one day  \n",
      "..                                                 ...  \n",
      "995  - \\nBTC price: $23,193 / £19,380 \\n\\n43.12 Nak...  \n",
      "996  El precio del #BTC: 23,176 USD / 21,905 EUR / ...  \n",
      "997  This zesty faucet from @_bitcoiner is making m...  \n",
      "998       @w_s_bitcoin I would also like to know this.  \n",
      "999  Been getting going back &amp; forth with this ...  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "query = \"Bitcoin until:2023-03-01 since:2023-01-01\"\n",
    "tweets = []\n",
    "limit = 1000\n",
    "\n",
    "from IPython.display import clear_output\n",
    "count=0\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    \n",
    "    # print(vars(tweet))\n",
    "    # break\n",
    "    clear_output(wait=True)\n",
    "    count+=1\n",
    "    print(count)\n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        tweets.append([tweet.date, tweet.username, tweet.content])\n",
    "        \n",
    "df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Amal\n",
      "[nltk_data]     Jose\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Amal Jose/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Amal Jose\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words.zip/words/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Amal Jose/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Amal Jose\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\AMALJO~1\\AppData\\Local\\Temp/ipykernel_12076/2977651530.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[0mstopwords_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[0mlem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('words')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/words\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Amal Jose/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Amal Jose\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#import logging\n",
    "import re\n",
    "#from collections import defaultdict\n",
    "#import phonenumbers\n",
    "#import pyap\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "#from itertools import zip_longest\n",
    "# from automation.sqlUtils import selectForWhereClause\n",
    "# from src.file_parsers.utils.fileHandler import *\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import xgboost as xgb\n",
    "#from wrapt_timeout_decorator import *\n",
    "\n",
    "\n",
    "\n",
    "#### Get the saved ML Model\n",
    "#### Get the save count vectoriser model\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    punctuations = '¯.,!™¥-♥#$%&:;\\()+-<=>@[\\\\]^_`{|}~/¯( ͡° ͜ʖ ͡°) ͜ʖ° ͜ʖ ͡°ʖ ͡☉ ͡☉ ͡ ͡ ͡  ͡ ▌░╔╗║╔═╗├│Γû╝├▒»ΓÇÖ'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p, ' ')\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(\" he's \", \" he is \", text)\n",
    "    text = re.sub(\" there's \", \" there is \", text)\n",
    "    text = re.sub(\" we're \", \" we are \", text)\n",
    "    text = re.sub(\"you'll\", \" you will \", text)\n",
    "    text = re.sub(\" that's \", \" that is \", text)\n",
    "    text = re.sub(\" won't \", \" will not \", text)\n",
    "    text = re.sub(\" they're \", \" they are \", text)\n",
    "    text = re.sub(\" wan't \", \" cannot \", text)\n",
    "    text = re.sub(\" wasn't \", \" was not \", text)\n",
    "    text = re.sub(\" aren't \", \" are not \", text)\n",
    "    text = re.sub(\" isn't \", \" is not \", text)\n",
    "    text = re.sub(\"haven't\", \" have not \", text)\n",
    "    text = re.sub(\" hasn't \", \" has not \", text)\n",
    "    text = re.sub(\" there's \", \" there is \", text)\n",
    "    text = re.sub(\" he's \", \" he is \", text)\n",
    "    text = re.sub(\" it's \", \" it is \", text)\n",
    "    text = re.sub(\" you're \", \" you are \", text)\n",
    "    text = re.sub(\" isnt \", \" is not \", text)\n",
    "    text = re.sub(\" shouldn't \", \" should not \", text)\n",
    "    text = re.sub(\" wouldn't \", \" would not \", text)\n",
    "    text = re.sub(\"i'm\", \" i am \", text)\n",
    "    text = re.sub(\"i'm \", \" i am \", text)\n",
    "    text = re.sub(\" im \", \" i am \", text)\n",
    "    text = re.sub(\" youre \", \" you are \", text)\n",
    "    text = re.sub(\" ive \", \" i have \", text)\n",
    "    text = re.sub(\" cant \", \" can not \", text)\n",
    "    text = re.sub(\" ive \", \" i have \", text)\n",
    "    text = re.sub(\" dont \", \" do not \", text)\n",
    "    text = re.sub(\"doesnt\", \" does not \", text)\n",
    "    text = re.sub(\" thats \", \" that is \", text)\n",
    "    text = re.sub(\" doesnt \", \" does not \", text)\n",
    "    text = re.sub(\" don ‘ t \", \" do not \", text)\n",
    "    text = re.sub(\" don't \", \" do not \", text)\n",
    "    text = re.sub(\" i'll\", \" i will \", text)\n",
    "    text = re.sub(\" isn't \", \" is not \", text)\n",
    "    text = re.sub(\" here's\", \" here is \", text)\n",
    "    text = re.sub(\" you've\", \" you have \", text)\n",
    "    text = re.sub(\" we're\", \" we are \", text)\n",
    "    text = re.sub(\" what's\", \" what is \", text)\n",
    "    text = re.sub(\" couldn't\", \"could not\", text)\n",
    "    text = re.sub(\" we've\", \" we have \", text)\n",
    "    text = re.sub(\" who's\", \" who is \", text)\n",
    "    text = re.sub(\" y'all\", \" you all \", text)\n",
    "    text = re.sub(\" would've\", \" would have \", text)\n",
    "    text = re.sub(\" it'll\", \" it will \", text)\n",
    "    text = re.sub(\" we'll\", \" we will \", text)\n",
    "    text = re.sub(\" we've\", \" we have \", text)\n",
    "    text = re.sub(\" he'll\", \" he will \", text)\n",
    "    text = re.sub(\" y'all\", \" you all \", text)\n",
    "    text = re.sub(\" weren't\", \" were not \", text)\n",
    "    text = re.sub(\" didn't\", \" did not \", text)\n",
    "    text = re.sub(\" they'll\", \" they will \", text)\n",
    "    text = re.sub(\" they'd\", \" they would \", text)\n",
    "    text = re.sub(\" they've\", \" they have \", text)\n",
    "    text = re.sub(\" i'd\", \" i would \", text)\n",
    "    text = re.sub(\" should've\", \" should have \", text)\n",
    "    text = re.sub(\" where's\", \" where is \", text)\n",
    "    text = re.sub(\" we'd\", \" we would \", text)\n",
    "    text = re.sub(\" weren't\", \" were not \", text)\n",
    "    text = re.sub(\" they're\", \" they are \", text)\n",
    "    text = re.sub(\" let's\", \" let us \", text)\n",
    "    text = re.sub(\" it's\", \" it is \", text)\n",
    "    text = re.sub(\" can't\", \" cannot \", text)\n",
    "    text = re.sub(\" don't\", \" do not \", text)\n",
    "    text = re.sub(\" you're\", \" you are \", text)\n",
    "    text = re.sub(\" i've\", \" i have \", text)\n",
    "    text = re.sub(\" that's\", \" that is \", text)\n",
    "    text = re.sub(\" doesn't\", \" does not \", text)\n",
    "    text = re.sub(\" i'd\", \" i would \", text)\n",
    "    text = re.sub(\" didn't\", \" did not \", text)\n",
    "    text = re.sub(\" ain't\", \" am not \", text)\n",
    "    text = re.sub(\" you'll\", \" you will \", text)\n",
    "    text = re.sub(\" i've\", \" i have \", text)\n",
    "    text = re.sub(\" don't\", \" do not \", text)\n",
    "    text = re.sub(\"i'll\", \" i will \", text)\n",
    "    text = re.sub(\" i'd\", \" i would \", text)\n",
    "    text = re.sub(\" let's\", \" let us \", text)\n",
    "    text = re.sub(\" you'd\", \" you would \", text)\n",
    "    text = re.sub(\" it's\", \" it is \", text)\n",
    "    text = re.sub(\" ain't\", \" am not \", text)\n",
    "    text = re.sub(\" could've\", \" could have \", text)\n",
    "    text = re.sub(\"youve\", \" you have \", text)\n",
    "    text = re.sub(\"i'm\", \" i am \", text)\n",
    "    #     Character entity references\n",
    "    text = re.sub(\"&gt;\", \">\", text)\n",
    "    text = re.sub(\"&lt;\", \"<\", text)\n",
    "    text = re.sub(\"&amp;\", \"&\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_phone_numbers(file):\n",
    "    phone_set = set()\n",
    "    for match in phonenumbers.PhoneNumberMatcher(file, \"US\"):\n",
    "        phone_set.add(phonenumbers.format_number(match.number, phonenumbers.PhoneNumberFormat.E164))\n",
    "    return phone_set\n",
    "\n",
    "\n",
    "def new_ad_count(body, rules):\n",
    "    soup = BeautifulSoup(body, features='html.parser')\n",
    "    #### Advertisment count in html\n",
    "    scripts = soup.find_all('script')\n",
    "    srcs = [link['src'] for link in scripts if 'src' in link.attrs]\n",
    "    cnt = 0\n",
    "    for src in srcs:\n",
    "        if rules.should_block(src) and not src == \"https://www.googletagservices.com/tag/js/gpt.js\":\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "\n",
    "def text_cleaning(text, stopwords_list, lem):\n",
    "#     words = set(words_list)\n",
    "    main_words = remove_punct(text.lower())\n",
    "    main_words = clean(main_words)\n",
    "    main_words = remove_emoji(main_words)\n",
    "    main_words = remove_URL(main_words)\n",
    "    main_words = main_words.split(\" \")\n",
    "    main_words = [w for w in main_words if len(w) > 1]\n",
    "    main_words = [w for w in main_words if not w in set(stopwords_list) and len(w) > 1]\n",
    "    main_words = [lem.lemmatize(w.lower().strip()) for w in main_words if len(w) > 1]\n",
    "    main_words = [w for w in main_words if w.isalpha()]\n",
    "    main_words = ' '.join([w.lower() for w in main_words if len(w) < 20 and len(w) > 1])\n",
    "    main_words = re.sub(r'[^\\x00-\\x7f]+', '', main_words)\n",
    "    main_words = re.sub(r\"[\\d\\.]\", '', main_words).replace(\"  \", \"\")\n",
    "    return main_words\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stopwords_list = stopwords.words('english')\n",
    "words = set(nltk.corpus.words.words())\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_tweet']=df['Tweet'].apply(lambda x: text_cleaning(x,stopwords_list,lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('twitter.csv',dtype=str).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.loc[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key =\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reponse(format_string,openai):\n",
    "    response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=format_string,\n",
    "  temperature=0,\n",
    "  max_tokens=60,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    "    )\n",
    "    resp=response[\"choices\"][0][\"text\"].split(\"\\n\")\n",
    "    resp_list=[]\n",
    "    for i in resp:\n",
    "        if \". Neutral\" in i:\n",
    "            resp_list.append(0)\n",
    "        if \". Positive\" in i:\n",
    "            resp_list.append(1)\n",
    "        if \". Negative\" in i:\n",
    "            resp_list.append(-1)\n",
    "    return resp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_format_string(list_of_tweets):\n",
    "    return \"Classify the sentiment in these tweets:\\n \" + \"\\n \".join([f\"{i+1}. {string}\" for i, string in enumerate(list_of_tweets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Clean_tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-28 23:59:59+00:00</td>\n",
       "      <td>jmbushwrites</td>\n",
       "      <td>Sound money matters.\\n\\n#Bitcoin is sound money.</td>\n",
       "      <td>sound money matter bitcoin sound money</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-28 23:59:56+00:00</td>\n",
       "      <td>cryptobits_org</td>\n",
       "      <td>Visa and Mastercard are pausing crypto product...</td>\n",
       "      <td>visa mastercard pausing crypto product report ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-28 23:59:52+00:00</td>\n",
       "      <td>romoolo</td>\n",
       "      <td>❌ do not ever bet against El Salvador 🇸🇻 nor #...</td>\n",
       "      <td>ever bet el salvador bitcoin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-28 23:59:52+00:00</td>\n",
       "      <td>BitcoinEnso</td>\n",
       "      <td>@trustmachinesco is dedicated to building and ...</td>\n",
       "      <td>trustmachinesco dedicated building innovating ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-28 23:59:51+00:00</td>\n",
       "      <td>AdamBitcoinP2P1</td>\n",
       "      <td>@windyseob We’ll share one one day</td>\n",
       "      <td>windyseob share one one day</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-02-28 23:59:50+00:00</td>\n",
       "      <td>lqdbtc</td>\n",
       "      <td>@_tnull Very interesting projects.  Are they o...</td>\n",
       "      <td>tnull interesting project open contribution no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-02-28 23:59:48+00:00</td>\n",
       "      <td>Gebie_1</td>\n",
       "      <td>I hope and i know that the whole rebranding wi...</td>\n",
       "      <td>hope know whole rebranding equally make team c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-02-28 23:59:46+00:00</td>\n",
       "      <td>WorldCoinIndex</td>\n",
       "      <td>Bitcoin price index https://t.co/o7UcHJUhC6 #U...</td>\n",
       "      <td>bitcoin price index http co usd eur cny gbp ru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-02-28 23:59:44+00:00</td>\n",
       "      <td>InvariantPersp1</td>\n",
       "      <td>#recession ... #Crypto Bubble edition\\n\\n#Bitc...</td>\n",
       "      <td>recession crypto bubble edition bitcoin btcusd...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-02-28 23:59:42+00:00</td>\n",
       "      <td>LOCALTRADERSCL</td>\n",
       "      <td>Want to buy #Bitcoin? \\n\\nIt’s just a tap away...</td>\n",
       "      <td>want buy tap away exchange buy payment method ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-02-28 23:59:39+00:00</td>\n",
       "      <td>AergiaOne</td>\n",
       "      <td>@zetablockchain is the world's first and only ...</td>\n",
       "      <td>zetablockchain first decentralized blockchain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-02-28 23:59:35+00:00</td>\n",
       "      <td>CnaAlx</td>\n",
       "      <td>💯#top100token.com/ Nova Listagem | @CnaAlx \\n\\...</td>\n",
       "      <td>com nova listagem cnaalx alxcna http co kaztxr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2023-02-28 23:59:29+00:00</td>\n",
       "      <td>greegameplay</td>\n",
       "      <td>This zesty faucet from @_bitcoiner is making m...</td>\n",
       "      <td>zesty faucet bitcoiner making tweet claim free...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2023-02-28 23:59:28+00:00</td>\n",
       "      <td>AdamBitcoinP2P1</td>\n",
       "      <td>@jiajinie24 Good morning fren 💜</td>\n",
       "      <td>good morning fren</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Date             User  \\\n",
       "0   2023-02-28 23:59:59+00:00     jmbushwrites   \n",
       "1   2023-02-28 23:59:56+00:00   cryptobits_org   \n",
       "2   2023-02-28 23:59:52+00:00          romoolo   \n",
       "3   2023-02-28 23:59:52+00:00      BitcoinEnso   \n",
       "4   2023-02-28 23:59:51+00:00  AdamBitcoinP2P1   \n",
       "5   2023-02-28 23:59:50+00:00           lqdbtc   \n",
       "6   2023-02-28 23:59:48+00:00          Gebie_1   \n",
       "7   2023-02-28 23:59:46+00:00   WorldCoinIndex   \n",
       "8   2023-02-28 23:59:44+00:00  InvariantPersp1   \n",
       "9   2023-02-28 23:59:42+00:00   LOCALTRADERSCL   \n",
       "10  2023-02-28 23:59:39+00:00        AergiaOne   \n",
       "11  2023-02-28 23:59:35+00:00           CnaAlx   \n",
       "12  2023-02-28 23:59:29+00:00     greegameplay   \n",
       "13  2023-02-28 23:59:28+00:00  AdamBitcoinP2P1   \n",
       "\n",
       "                                                Tweet  \\\n",
       "0    Sound money matters.\\n\\n#Bitcoin is sound money.   \n",
       "1   Visa and Mastercard are pausing crypto product...   \n",
       "2   ❌ do not ever bet against El Salvador 🇸🇻 nor #...   \n",
       "3   @trustmachinesco is dedicated to building and ...   \n",
       "4                  @windyseob We’ll share one one day   \n",
       "5   @_tnull Very interesting projects.  Are they o...   \n",
       "6   I hope and i know that the whole rebranding wi...   \n",
       "7   Bitcoin price index https://t.co/o7UcHJUhC6 #U...   \n",
       "8   #recession ... #Crypto Bubble edition\\n\\n#Bitc...   \n",
       "9   Want to buy #Bitcoin? \\n\\nIt’s just a tap away...   \n",
       "10  @zetablockchain is the world's first and only ...   \n",
       "11  💯#top100token.com/ Nova Listagem | @CnaAlx \\n\\...   \n",
       "12  This zesty faucet from @_bitcoiner is making m...   \n",
       "13                    @jiajinie24 Good morning fren 💜   \n",
       "\n",
       "                                          Clean_tweet  Sentiment  \n",
       "0              sound money matter bitcoin sound money          0  \n",
       "1   visa mastercard pausing crypto product report ...          0  \n",
       "2                        ever bet el salvador bitcoin          1  \n",
       "3   trustmachinesco dedicated building innovating ...          1  \n",
       "4                         windyseob share one one day          0  \n",
       "5   tnull interesting project open contribution no...          1  \n",
       "6   hope know whole rebranding equally make team c...          1  \n",
       "7   bitcoin price index http co usd eur cny gbp ru...          0  \n",
       "8   recession crypto bubble edition bitcoin btcusd...         -1  \n",
       "9   want buy tap away exchange buy payment method ...          1  \n",
       "10  zetablockchain first decentralized blockchain ...          1  \n",
       "11  com nova listagem cnaalx alxcna http co kaztxr...          0  \n",
       "12  zesty faucet bitcoiner making tweet claim free...          1  \n",
       "13                                  good morning fren          1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df=pd.read_csv('twitter.csv',dtype=str).fillna(\"\")  ### READ IF SAVED IT IN LOCAL\n",
    "df=df.loc[:13]  ### GETS FOR FIRST 14 TWEETS\n",
    "format_string=create_format_string(df['Clean_tweet'])\n",
    "responses=get_reponse(format_string,openai)\n",
    "if len(responses)<len(df['Clean_tweet']):\n",
    "    misses=len(df['Clean_tweet'])-len(responses)\n",
    "    for i in range(misses):\n",
    "        responses.append(\"\")\n",
    "df['Sentiment']=responses\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
